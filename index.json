[{"authors":["admin"],"categories":null,"content":"Hi! I\u0026rsquo;m a Ph.D. student in EECS at U.C. Berkeley advised by Niloufar Salehi and Moritz Hardt. My research is at the intersection of human-computer interaction and machine learning. I\u0026rsquo;m especially excited about human-centered and participatory approaches to designing algorithmic systems that are transparent, contestable, and responsive to community values and needs. At the moment I\u0026rsquo;m studying how families engage with student assignment algorithms for enrolling in public schools, and how users calibrate trust in machine translation systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Hi! I\u0026rsquo;m a Ph.D. student in EECS at U.C. Berkeley advised by Niloufar Salehi and Moritz Hardt. My research is at the intersection of human-computer interaction and machine learning. I\u0026rsquo;m especially excited about human-centered and participatory approaches to designing algorithmic systems that are transparent, contestable, and responsive to community values and needs. At the moment I\u0026rsquo;m studying how families engage with student assignment algorithms for enrolling in public schools, and how users calibrate trust in machine translation systems.","tags":null,"title":"Sam Robertson","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536476400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536476400,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Samantha Robertson","Wesley Hanwen Deng","Timnit Gebru","Margaret Mitchell","Daniel J. Liebling","Michal Lahav","Katherine Heller","Mark Díaz","Samy Bengio","Niloufar Salehi"],"categories":null,"content":"","date":1618902000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618902000,"objectID":"29267f5f1d318b3e61c4ea1c84435dda","permalink":"/publication/hcmt/","publishdate":"2021-04-20T00:00:00-07:00","relpermalink":"/publication/hcmt/","section":"publication","summary":"As people all over the world adopt machine translation (MT) to communicate across languages, there is increased need for affordances that aid users in understanding when to rely on automated translations. Identifying the information and interactions that will most help users meet their translation needs is an open area of research at the intersection of Human-Computer Interaction (HCI) and Natural Language Processing (NLP). This paper advances work in this area by drawing on a survey of users' strategies in assessing translations. We identify three directions for the design of translation systems that support more reliable and effective use of machine translation: helping users craft good inputs, helping users understand translations, and expanding interactivity and adaptivity. We describe how these can be introduced in current MT systems and highlight open questions for HCI and NLP research.","tags":["Machine Translation","Human-Centered Design"],"title":"Three Directions for the Design of Human-Centered Machine Translation","type":"publication"},{"authors":["Samantha Robertson","Tonya Nguyen","Niloufar Salehi"],"categories":null,"content":"","date":1611820800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611820800,"objectID":"b6061c9d7d5826816f4f189a3d6499d2","permalink":"/publication/modeling_assumptions_clash/","publishdate":"2021-01-28T00:00:00-08:00","relpermalink":"/publication/modeling_assumptions_clash/","section":"publication","summary":"Across the United States, a growing number of school districts are turning to matching algorithms to assign students to public schools. The designers of these algorithms aimed to promote values such as transparency, equity, and community in the process. However, school districts have encountered practical challenges in their deployment. In fact, San Francisco Unified School District voted to stop using and completely redesign their student assignment algorithm because it was frustrating for families and it was not promoting educational equity in practice. We analyze this system using a Value Sensitive Design approach and find that one reason values are not met in practice is that the system relies on modeling assumptions about families’ priorities, constraints, and goals that clash with the real world. These assumptions overlook the complex barriers to ideal participation that many families face, particularly because of socioeconomic inequalities. We argue that direct, ongoing engagement with stakeholders is central to aligning algorithmic values with real world conditions. In doing so we must broaden how we evaluate algorithms while recognizing the limitations of purely algorithmic solutions in addressing complex socio-political problems.","tags":["Student assignment","Mechanism design","Value sensitive design"],"title":"Modeling Assumptions Clash with the Real World: Transparency, Equity, and Community Challenges for Student Assignment Algorithms","type":"publication"},{"authors":["Samantha Robertson","Niloufar Salehi"],"categories":null,"content":"","date":1591772400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591772400,"objectID":"ac9f026841593a10b77fb32448d9f591","permalink":"/publication/limits_of_prefs/","publishdate":"2020-06-10T00:00:00-07:00","relpermalink":"/publication/limits_of_prefs/","section":"publication","summary":"Emerging methods for participatory algorithm design have proposed collecting and aggregating individual stakeholder preferences to create algorithmic systems that account for those stakeholders' values. Using algorithmic student assignment as a case study, we argue that optimizing for individual preference satisfaction in the distribution of limited resources may actually inhibit progress towards social and distributive justice. Individual preferences can be a useful signal but should be expanded to support more expressive and inclusive forms of democratic participation.","tags":["matching algorithms","school choice","participatory algorithm design"],"title":"What If I Don't Like Any Of The Choices? The Limits of Preference Elicitation for Participatory Algorithm Design","type":"publication"},{"authors":null,"categories":[],"content":" Each year, the World Health Organization releases a report with estimates of global immunization coverage. The report lots of visualizations of their data, and they release the data used for each visualization publicly. About a year ago, fresh out of Data Challenge Lab, the class offered by the Stanford Data Lab which I then TA’d a year later, I decided to practice some of my new vis skills by trying to replicate some of the visualizations in the 2016 report.\nI replicated three visualizations. Below are the comparisons of WHO’s vis against mine. All the code to create my plots is at the bottom of this post! I’m not fully convinced that the WHO visualizations are the most effective, for example two of them use redundant color, but for the challenge I tried to replicate them as faithfully as possible.\nBelow is the code for the three plots my way!\nFirst, I load the data and libraries and define some functions and constants for all three plots.\n# Libraries library(tidyverse) # Files data_dir \u0026lt;- \u0026quot;~/Data/who-immunizations/\u0026quot; file_global_mcv \u0026lt;- str_c(data_dir, \u0026quot;global_regional_coverage.csv\u0026quot;) file_weunic \u0026lt;- str_c(data_dir, \u0026quot;wuenic_master_07_06_2017.csv\u0026quot;) file_subnational \u0026lt;- str_c(data_dir, \u0026quot;subnational_06_29_2017.csv\u0026quot;) # Constants national_dtp3_countries \u0026lt;- c( \u0026quot;Pakistan\u0026quot;, \u0026quot;Syrian Arab Republic\u0026quot;, \u0026quot;Yemen\u0026quot;, \u0026quot;Iraq\u0026quot;, \u0026quot;Mali\u0026quot;, \u0026quot;Afghanistan\u0026quot;, \u0026quot;Haiti\u0026quot;, \u0026quot;Ethiopia\u0026quot;, \u0026quot;Democratic Republic of the Congo\u0026quot;, \u0026quot;Nigeria\u0026quot;, \u0026quot;Somalia\u0026quot; ) subnat_coverage_order \u0026lt;- c( \u0026quot;0 to 60%\u0026quot;, \u0026quot;60% to 70%\u0026quot;, \u0026quot;70% to 80%\u0026quot;, \u0026quot;90% to 95%\u0026quot;, \u0026quot;80% to 90%\u0026quot;, \u0026quot;95% to 100%\u0026quot;, \u0026quot;\u0026gt;100%\u0026quot; ) subnat_x_breaks \u0026lt;- c(seq(0, 100, by = 10), 400, 700, 1000) subnat_y_breaks \u0026lt;- c( 10, 500, 2000, 5000, 10000, 15000, seq(20000, 60000, by = 10000), 80000, 100000, 150000, seq(200000, 500000, by = 100000) ) subnat_size_breaks \u0026lt;- c(1, 10, 100, 1000, 10000, 100000, 300000) subnational_dtp3_labels \u0026lt;- c(\u0026quot;Dhaka\u0026quot;, \u0026quot;Lahore\u0026quot;, \u0026quot;Karachi\u0026quot;, \u0026quot;São Paulo\u0026quot;) # I picked the colors from the original plots using an online color picker color_mcv1 \u0026lt;- \u0026quot;#69bcd1\u0026quot; color_mcv2 \u0026lt;- \u0026quot;#d13f3e\u0026quot; color_mcv_refline \u0026lt;- \u0026quot;#367cc1\u0026quot; colors_dtp3_subnational \u0026lt;- c( \u0026quot;0 to 60%\u0026quot; = \u0026quot;#d5322f\u0026quot;, \u0026quot;60% to 70%\u0026quot; = \u0026quot;#f36d4a\u0026quot;, \u0026quot;70% to 80%\u0026quot; = \u0026quot;#fbad68\u0026quot;, \u0026quot;80% to 90%\u0026quot; = \u0026quot;#92cc64\u0026quot;, \u0026quot;90% to 95%\u0026quot; = \u0026quot;#6abc68\u0026quot;, \u0026quot;95% to 100%\u0026quot; = \u0026quot;#249752\u0026quot;, \u0026quot;\u0026gt;100%\u0026quot; = \u0026quot;#876086\u0026quot; ) # Helper functions global_mcv_y_labels \u0026lt;- function(vals) { # Add the percent sign only to the very top (100%) label if_else(vals == 100, str_c(vals, \u0026quot;%\u0026quot;), str_c(vals)) } subnat_x_labels \u0026lt;- function(vals) { # Add the percent sign to all but make the last element \u0026gt;X% vals \u0026lt;- scales::percent(vals, accuracy = 1, scale = 1) vals[length(vals)] \u0026lt;- str_c(\u0026quot;\u0026gt;\u0026quot;, vals[length(vals)]) vals } subnat_size_labels \u0026lt;- function(vals) { if_else(vals \u0026gt; 100, str_c(vals / 1000, \u0026quot;k\u0026quot;), str_c(vals)) } x_trans_trans \u0026lt;- function(x) { # Custom axis transformation for subnational dtp3 plot scales::trans_new( \u0026quot;x_trans\u0026quot;, function(x) if_else(x \u0026lt;= 100, x, 93.5 + (x/15)), function(x) if_else(x \u0026lt;= 100, x, (x - 93.5) * 15) ) } # ============================================================================== global_mcv \u0026lt;- read_csv(file_global_mcv) weunic \u0026lt;- read_csv(file_weunic) subnational \u0026lt;- read_csv(file_subnational) Global MCV Coverage This visualization shows the percentage of children worldwide receiving the MCV1 and MCV2 vaccines between 2000 and 2016. Vaccination coverage has been steadily increasing for both vaccines, though neither has reached the 90% target. MCV2 coverage is lower overall than MCV1 coverage, but is increasing at a faster rate.\nglobal_mcv %\u0026gt;% rename_all(str_to_lower) %\u0026gt;% filter( group == \u0026quot;Global\u0026quot;, vaccine %in% c(\u0026quot;mcv1\u0026quot;, \u0026quot;mcv2\u0026quot;), year \u0026gt;= 2000 ) %\u0026gt;% ggplot() + geom_line(aes(year, coverage, color = vaccine), size = 0.8) + geom_hline(yintercept = 90, color = color_mcv_refline, size = 0.8) + annotate( geom = \u0026quot;text\u0026quot;, x = 2008, y = 93, hjust = 0.5, label = \u0026quot;90% Vaccination Target\u0026quot;, color = color_mcv_refline, size = 3, fontface = \u0026quot;bold\u0026quot; ) + scale_x_continuous( breaks = seq(2000, 2016, by = 2) ) + scale_y_continuous( breaks = seq(0, 100, by = 20), limits = c(0, 100), labels = global_mcv_y_labels ) + scale_color_manual( values = c(\u0026quot;mcv1\u0026quot; = color_mcv1, \u0026quot;mcv2\u0026quot; = color_mcv2), labels = str_to_upper ) + theme_minimal() + theme( plot.title = element_text(face = \u0026quot;bold\u0026quot;), legend.justification = c(\u0026quot;right\u0026quot;, \u0026quot;top\u0026quot;), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(), panel.background = element_rect(color = \u0026quot;grey60\u0026quot;), panel.grid.minor.y = element_line(color = \u0026quot;grey60\u0026quot;, size = 0.2), panel.grid.major.y = element_line(color = \u0026quot;grey60\u0026quot;, size = 0.2) ) + coord_cartesian( xlim = c(1999.5, 2016.5), ylim = c(0, 100), expand = FALSE, ) + labs( x = NULL, y = \u0026quot;Coverage\u0026quot;, color = NULL, title = \u0026quot;Global MCV1 and MCV2 Coverage\u0026quot; )  National DTP3 Coverage This visualization shows the change between 2010 and 2016 in DTP3 coverage in select countries. Like in the next visualization (subnational DTP3 coverage), color is encoding coverage in 2016, which is a redundant encoding. I did find that the graph is much more pleasing to the eye with the encoding, but is much simpler to understand without it, in my opinion.\nThere was a little bit of data manipulation to be done for this plot. The first step is filtering to the vaccine, countries and years we want. Then there is a mutate to create some helpful plotting variables. I created the tibble weunic_country_colors to keep track of which country should be which color.\nweunic \u0026lt;- weunic %\u0026gt;% rename_all(str_to_lower) %\u0026gt;% filter( year %in% c(2010, 2016), vaccine == \u0026quot;dtp3\u0026quot;, country %in% national_dtp3_countries ) %\u0026gt;% select(wuenic, year, country) %\u0026gt;% arrange(year, wuenic) %\u0026gt;% mutate( diff_to_next = lead(wuenic) - wuenic, ynudge = case_when( diff_to_next == 1 ~ -1, diff_to_next == 0 ~ -2, TRUE ~ 0 ), ypos = wuenic + ynudge ) weunic_country_colors \u0026lt;- weunic %\u0026gt;% filter(year == 2016) %\u0026gt;% mutate( color = case_when( wuenic \u0026lt; 60 ~ \u0026quot;red\u0026quot;, wuenic \u0026lt; 70 ~ \u0026quot;orange\u0026quot;, TRUE ~ \u0026quot;yellow\u0026quot; ) ) %\u0026gt;% select(country, color) weunic \u0026lt;- weunic %\u0026gt;% left_join(weunic_country_colors, by = \u0026quot;country\u0026quot;) Now the data is ready to plot! The code is long mostly because there are four labels for each country, and each needs to be a different type face and justification! The rest of the plot is quite basic.\nweunic %\u0026gt;% ggplot() + geom_point(aes(year, wuenic, color = color)) + geom_segment( aes( y = `2010`, yend = `2016`, color = color ), x = 2010, xend = 2016, data = select(weunic, country, year, wuenic) %\u0026gt;% spread(year, wuenic) %\u0026gt;% left_join(weunic_country_colors, by = \u0026quot;country\u0026quot;) ) + geom_text( # Label points with the numeric value aes( y = ypos, label = wuenic ), x = 2009.5, fontface = \u0026quot;bold\u0026quot;, data = filter(weunic, year == 2010) ) + geom_text( # Label points with country name aes( y = ypos, label = country ), x = 2009, hjust = 1, data = filter(weunic, year == 2010) ) + geom_text( aes( y = ypos, label = wuenic ), x = 2016.5, data = filter(weunic, year == 2016), fontface = \u0026quot;bold\u0026quot; ) + geom_text( aes( y = ypos, label = country ), x = 2017, hjust = 0, data = filter(weunic, year == 2016) ) + scale_x_continuous( breaks = c(2010, 2016), limits = c(2000, 2026), position = \u0026quot;top\u0026quot; ) + scale_color_manual( values = c( \u0026quot;red\u0026quot; = \u0026quot;#d5322f\u0026quot;, \u0026quot;orange\u0026quot; = \u0026quot;#f36d4a\u0026quot;, \u0026quot;yellow\u0026quot; = \u0026quot;#fbad68\u0026quot; ) ) + theme_minimal() + theme( panel.grid = element_blank(), axis.text.y = element_blank(), axis.text.x = element_text( color = \u0026quot;black\u0026quot;, size = 16, face = \u0026quot;bold\u0026quot; ), plot.title = element_text( size = 16, face = \u0026quot;bold\u0026quot;, hjust = 0.5 ), legend.position = \u0026quot;none\u0026quot; ) + labs( x = NULL, y = NULL, title = \u0026quot;Trends in DTPcv3 Coverage since\\n2010 for Selected Countries\u0026quot; )  Subnational DTP3 Coverage This visualization shows the 2016 Coverage rate and number of surviving infants at the subnational level across the world. I was struck by the aesthetics of the plot, however I am torn as to whether the redundant encoding of size and color is distracting or not. I have been taught that all redundant information should be excluded, and quite clearly each variable is encoded in two different aesthetics, however I do think the colors, at least, help to differentiate and draw the eye. Regardless, the plot was quite a challenge to replicate so I kept the redundancy just for fun.\nsubnational %\u0026gt;% filter( annum == 2016, Vaccode == \u0026quot;DTP3\u0026quot;, !is.na(Admin2) ) %\u0026gt;% mutate( color = case_when( Coverage \u0026lt;= 60 ~ \u0026quot;0 to 60%\u0026quot;, Coverage \u0026lt;= 70 ~ \u0026quot;60% to 70%\u0026quot;, Coverage \u0026lt;= 80 ~ \u0026quot;70% to 80%\u0026quot;, Coverage \u0026lt;= 90 ~ \u0026quot;80% to 90%\u0026quot;, Coverage \u0026lt;= 95 ~ \u0026quot;90% to 95%\u0026quot;, Coverage \u0026lt;= 100 ~ \u0026quot;95% to 100%\u0026quot;, TRUE ~ \u0026quot;\u0026gt;100%\u0026quot; ), color = factor(color, levels = subnat_coverage_order, ordered = TRUE), Coverage = if_else(Coverage \u0026lt; 1000, Coverage, 1000), label = if_else(Admin2 %in% subnational_dtp3_labels, Admin2, \u0026quot;\u0026quot;) ) %\u0026gt;% sample_frac() %\u0026gt;% ggplot() + geom_point( aes( Coverage, Denominator, size = Denominator, fill = color ), shape = 21, color = \u0026quot;white\u0026quot;, stroke = 0.25 ) + ggrepel::geom_text_repel( aes( Coverage, Denominator, label = label ), point.padding = 0.5, min.segment.length = 1 ) + scale_x_continuous( trans = \u0026quot;x_trans\u0026quot;, breaks = subnat_x_breaks, labels = subnat_x_labels, position = \u0026quot;top\u0026quot; ) + scale_y_continuous( trans = \u0026quot;sqrt\u0026quot;, breaks = subnat_y_breaks, labels = scales::unit_format(unit = \u0026quot;\u0026quot;, scale = 1, sep = \u0026quot;\u0026quot;), position = \u0026quot;right\u0026quot;, limits = c(10, 500000) ) + scale_size( range = c(1, 10), breaks = subnat_size_breaks, labels = subnat_size_labels, guide = guide_legend( title.position = \u0026quot;top\u0026quot;, nrow = 1, override.aes = list(fill = \u0026quot;black\u0026quot;, color = \u0026quot;black\u0026quot;), label.position = \u0026quot;bottom\u0026quot;, label.hjust = 0.5 ) ) + scale_fill_manual( values = colors_dtp3_subnational, guide = guide_legend( title.position = \u0026quot;top\u0026quot;, ncol = 1, override.aes = list(shape = 22, size = 5), reverse = TRUE ) ) + labs( x = \u0026quot;DTP3 Coverage\u0026quot;, y = \u0026quot;Surviving Infants\u0026quot;, title = \u0026quot;DTPcv3 Reported Coverage by District\u0026quot;, fill = \u0026quot;Coverage\u0026quot;, size = \u0026quot;Surviving Infants\u0026quot; ) + theme_minimal() + theme( axis.title.x = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0), axis.title.y = element_text(face = \u0026quot;bold\u0026quot;, hjust = 0), legend.position = \u0026quot;bottom\u0026quot;, legend.title = element_text(face = \u0026quot;bold\u0026quot;), legend.justification = \u0026quot;left\u0026quot;, plot.title = element_text(face = \u0026quot;bold\u0026quot;), panel.grid.minor = element_blank() ) + coord_cartesian( xlim = c(10, 1000), ylim = c(0, 500000), expand = FALSE, clip = \u0026quot;off\u0026quot; )  ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"dd08d9566fabafc6365a60f91b12ef6d","permalink":"/post/2019-08-09-who-immunizations/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/post/2019-08-09-who-immunizations/","section":"post","summary":"Each year, the World Health Organization releases a report with estimates of global immunization coverage. The report lots of visualizations of their data, and they release the data used for each visualization publicly. About a year ago, fresh out of Data Challenge Lab, the class offered by the Stanford Data Lab which I then TA’d a year later, I decided to practice some of my new vis skills by trying to replicate some of the visualizations in the 2016 report.","tags":["R","data-visualization","tidyverse"],"title":"Replicating WHO Visualizations of Global Immunization Coverage","type":"post"},{"authors":null,"categories":[],"content":" I started this jupyter notebook to help me learn to (1) use both Python and R in one Jupyter notebook and (2) translate tidyverse syntax into Python (specifically pandas \u0026amp; seaborn) syntax. I figured I would post it in the hopes it might be helpful to someone else. Also, I wanted to learn how to post a jupyter notebook on this website.\nHelpful links:\n Rmagic Seaborn Tidyverse -\u0026gt; Pandas (Most of this is actually taken straight from this post) R + Jupyter This link helped me set up my environment so that I could use R and Python in one notebook.  Setup import warnings warnings.filterwarnings('ignore') %reload_ext rpy2.ipython %R library(tidyverse)  array(['bindrcpp', 'dplyr', 'purrr', 'readr', 'tidyr', 'tibble', 'ggplot2', 'tidyverse', 'tools', 'stats', 'graphics', 'grDevices', 'utils', 'datasets', 'methods', 'base'], dtype='|S9')  # Load in the pandas library import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline  Make some fake data df = pd.DataFrame({'Alphabet': ['a', 'b', 'c', 'd','e', 'f', 'g', 'h','i'], 'A': [4, 3, 5, 2, 1, 7, 7, 5, 9], 'B': [0, 4, 3, 6, 7, 10,11, 9, 13], 'C': [1, 2, 3, 1, 2, 3, 1, 2, 3]})  Glimpse \u0026amp; Summarize %R glimpse(df)  function (x, df1, df2, ncp, log = FALSE)  df.head()    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     0 4 a 0 1   1 3 b 4 2   2 5 c 3 3   3 2 d 6 1   4 1 e 7 2     %%R -i df summary(df)   A Alphabet B C Min. :1.000 a :1 Min. : 0 Min. :1 1st Qu.:3.000 b :1 1st Qu.: 4 1st Qu.:1 Median :5.000 c :1 Median : 7 Median :2 Mean :4.778 d :1 Mean : 7 Mean :2 3rd Qu.:7.000 e :1 3rd Qu.:10 3rd Qu.:3 Max. :9.000 f :1 Max. :13 Max. :3 (Other):3  df.info(null_counts = True)  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 9 entries, 0 to 8 Data columns (total 4 columns): A 9 non-null int64 Alphabet 9 non-null object B 9 non-null int64 C 9 non-null int64 dtypes: int64(3), object(1) memory usage: 360.0+ bytes  Filtering %R df %\u0026gt;% filter(A \u0026gt; 2)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     1 4 a 0 1   2 3 b 4 2   3 5 c 3 3   4 7 f 10 3   5 7 g 11 1   6 5 h 9 2   7 9 i 13 3     df[df.A \u0026gt; 2]    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     0 4 a 0 1   1 3 b 4 2   2 5 c 3 3   5 7 f 10 3   6 7 g 11 1   7 5 h 9 2   8 9 i 13 3     Slice \u0026amp; Select %R df %\u0026gt;% slice(4:5)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     1 2 d 6 1   2 1 e 7 2     df.loc[3:4,:] # Note: if slicing only one row, use df.loc[[3],:]    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     3 2 d 6 1   4 1 e 7 2     %R df %\u0026gt;% select(A:B)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B     0 4 a 0   1 3 b 4   2 5 c 3   3 2 d 6   4 1 e 7   5 7 f 10   6 7 g 11   7 5 h 9   8 9 i 13     df.loc[:, \u0026quot;A\u0026quot;:\u0026quot;B\u0026quot;] # Select non-adjacent columns with df.loc[:, [\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;]]    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B     0 4 a 0   1 3 b 4   2 5 c 3   3 2 d 6   4 1 e 7   5 7 f 10   6 7 g 11   7 5 h 9   8 9 i 13     %R df %\u0026gt;% select(-A)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Alphabet B C     0 a 0 1   1 b 4 2   2 c 3 3   3 d 6 1   4 e 7 2   5 f 10 3   6 g 11 1   7 h 9 2   8 i 13 3     df.drop(labels = [\u0026quot;A\u0026quot;], axis = 1) # Can use columns = [\u0026quot;A\u0026quot;] in Python 3 / Pandas 0.21.0.    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    Alphabet B C     0 a 0 1   1 b 4 2   2 c 3 3   3 d 6 1   4 e 7 2   5 f 10 3   6 g 11 1   7 h 9 2   8 i 13 3     Arrange / Sort %R df %\u0026gt;% arrange(desc(A))    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     1 9 i 13 3   2 7 f 10 3   3 7 g 11 1   4 5 c 3 3   5 5 h 9 2   6 4 a 0 1   7 3 b 4 2   8 2 d 6 1   9 1 e 7 2     df.sort_values(by=\u0026quot;A\u0026quot;, ascending=False)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C     8 9 i 13 3   5 7 f 10 3   6 7 g 11 1   2 5 c 3 3   7 5 h 9 2   0 4 a 0 1   1 3 b 4 2   3 2 d 6 1   4 1 e 7 2     Mutate %R df %\u0026gt;% mutate(AoverC = A / C, Bplus1 = B + 1)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C AoverC Bplus1     1 4 a 0 1 4.000000 1.0   2 3 b 4 2 1.500000 5.0   3 5 c 3 3 1.666667 4.0   4 2 d 6 1 2.000000 7.0   5 1 e 7 2 0.500000 8.0   6 7 f 10 3 2.333333 11.0   7 7 g 11 1 7.000000 12.0   8 5 h 9 2 2.500000 10.0   9 9 i 13 3 3.000000 14.0     df.assign(AoverC = df.A / df.C, Bplus1 = lambda df: df[\u0026quot;B\u0026quot;] + 1 )    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C AoverC Bplus1     0 4 a 0 1 4.000000 1   1 3 b 4 2 1.500000 5   2 5 c 3 3 1.666667 4   3 2 d 6 1 2.000000 7   4 1 e 7 2 0.500000 8   5 7 f 10 3 2.333333 11   6 7 g 11 1 7.000000 12   7 5 h 9 2 2.500000 10   8 9 i 13 3 3.000000 14     To use functions that are NOT vectorized\n%%R -i df is_b \u0026lt;- function(letter) { letter == \u0026quot;b\u0026quot; } # Pretend this function doesn't work on vectors df %\u0026gt;% rowwise %\u0026gt;% mutate(is_Alphabet_b = is_b(Alphabet))  Source: local data frame [9 x 5] Groups: \u0026lt;by row\u0026gt; # A tibble: 9 x 5 A Alphabet B C is_Alphabet_b \u0026lt;int\u0026gt; \u0026lt;fctr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;lgl\u0026gt; 1 4 a 0 1 FALSE 2 3 b 4 2 TRUE 3 5 c 3 3 FALSE 4 2 d 6 1 FALSE 5 1 e 7 2 FALSE 6 7 f 10 3 FALSE 7 7 g 11 1 FALSE 8 5 h 9 2 FALSE 9 9 i 13 3 FALSE  def is_b(letter): return letter == \u0026quot;b\u0026quot; # Again, pretend not vectorized df.assign( is_Alphabet_b = lambda df: df.Alphabet.apply(is_b) )    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C is_Alphabet_b     0 4 a 0 1 False   1 3 b 4 2 True   2 5 c 3 3 False   3 2 d 6 1 False   4 1 e 7 2 False   5 7 f 10 3 False   6 7 g 11 1 False   7 5 h 9 2 False   8 9 i 13 3 False     Grouping \u0026amp; Summarizing %%R -i df df %\u0026gt;% group_by(C) %\u0026gt;% summarize(avg_A = mean(A), mean_B = mean(B))  # A tibble: 3 x 3 C avg_A mean_B \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 1 4.333333 5.666667 2 2 3.000000 6.666667 3 3 7.000000 8.666667  df.groupby(\u0026quot;C\u0026quot;)\\ .agg({'A' : ['mean'], 'B' : ['mean']})    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }     A B    mean mean   C       1 4.333333 5.666667   2 3.000000 6.666667   3 7.000000 8.666667     This is super nasty. I hope I can find something better or will need to practice with this a lot.\nJoins First I\u0026rsquo;ll create a second fake dataset to join\ndf2 = pd.DataFrame({'Group': [\u0026quot;First\u0026quot;, \u0026quot;Second\u0026quot;, \u0026quot;Third\u0026quot;], 'C': [1, 2, 3]})  %%R -i df,df2 df %\u0026gt;% left_join(df2, by = \u0026quot;C\u0026quot;)   A Alphabet B C Group 1 4 a 0 1 First 2 3 b 4 2 Second 3 5 c 3 3 Third 4 2 d 6 1 First 5 1 e 7 2 Second 6 7 f 10 3 Third 7 7 g 11 1 First 8 5 h 9 2 Second 9 9 i 13 3 Third  df.merge(df2, how = \u0026quot;left\u0026quot;, on = \u0026quot;C\u0026quot;)    .dataframe thead tr:only-child th { text-align: right; } .dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }    A Alphabet B C Group     0 4 a 0 1 First   1 3 b 4 2 Second   2 5 c 3 3 Third   3 2 d 6 1 First   4 1 e 7 2 Second   5 7 f 10 3 Third   6 7 g 11 1 First   7 5 h 9 2 Second   8 9 i 13 3 Third     Plotting %%R -i df df %\u0026gt;% ggplot() + geom_point(aes(x=A, y=B, color=factor(C)), size = 2)  sns.set(style=\u0026quot;darkgrid\u0026quot;) sns.relplot(x=\u0026quot;A\u0026quot;, y=\u0026quot;B\u0026quot;, hue=\u0026quot;C\u0026quot;, data=df, palette = sns.color_palette(n_colors = 3))  \u0026lt;seaborn.axisgrid.FacetGrid at 0x11cd56e10\u0026gt;  ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"3446b8a612a605d8e211694ba5a4f65e","permalink":"/post/2019-08-08-tidyversetopandas/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/2019-08-08-tidyversetopandas/","section":"post","summary":"I started this jupyter notebook to help me learn to (1) use both Python and R in one Jupyter notebook and (2) translate tidyverse syntax into Python (specifically pandas \u0026amp; seaborn) syntax. I figured I would post it in the hopes it might be helpful to someone else. Also, I wanted to learn how to post a jupyter notebook on this website.\nHelpful links:\n Rmagic Seaborn Tidyverse -\u0026gt; Pandas (Most of this is actually taken straight from this post) R + Jupyter This link helped me set up my environment so that I could use R and Python in one notebook.","tags":["R","tidyverse","python","jupyter"],"title":"Translating Tidyverse to Python","type":"post"},{"authors":null,"categories":[],"content":" This week’s RWeekly digest included an awesome dataset from the World Bank that documents the percentage of women in lower house parliament across the globe from 1997 to 2018. The dataset is nearly complete and also includes summary statistics for numerous geographic aggregations.\nAll of my code to process and explore the data is available on my GitHub. There’s lots of interesting things to look at in this data, so I will leave most of the boring stuff out of this post.\nTo get a sense of the World Bank data, here are the top 10 and bottom 10 countries in 2018 in terms of percentage of parliament made up of women.\nwip %\u0026gt;% filter(year == 2018) %\u0026gt;% top_n(n = 10, wt = prop_women_seats) %\u0026gt;% arrange(desc(prop_women_seats)) %\u0026gt;% knitr::kable(caption = \u0026quot;Top 10 Countries with the largest proportion of women in their parliament in 2018\u0026quot;)  Table 1: Top 10 Countries with the largest proportion of women in their parliament in 2018  country_name country_code year prop_women_seats    Rwanda RWA 2018 61.3  Cuba CUB 2018 53.2  Bolivia BOL 2018 53.1  Mexico MEX 2018 48.2  Grenada GRD 2018 46.7  Namibia NAM 2018 46.2  Sweden SWE 2018 46.1  Nicaragua NIC 2018 45.7  Costa Rica CRI 2018 45.6  South Africa ZAF 2018 42.3    wip %\u0026gt;% filter(year == 2018) %\u0026gt;% top_n(n = 10, wt = -prop_women_seats) %\u0026gt;% arrange(prop_women_seats) %\u0026gt;% knitr::kable(caption = \u0026quot;Bottom 10 Countries with the smallest proportion of women in their parliament in 2018\u0026quot;)  Table 2: Bottom 10 Countries with the smallest proportion of women in their parliament in 2018  country_name country_code year prop_women_seats    Micronesia, Fed. Sts. FSM 2018 0.0  Papua New Guinea PNG 2018 0.0  Vanuatu VUT 2018 0.0  Yemen, Rep. YEM 2018 0.0  Oman OMN 2018 1.2  Haiti HTI 2018 2.5  Kuwait KWT 2018 3.1  Solomon Islands SLB 2018 4.0  Lebanon LBN 2018 4.7  Thailand THA 2018 5.3    Only 3 countries had over half women in their parliament in 2018! The top 10 are still all over 40%, which seems encouraging! Less encouragingly, four nations still had 0 women in the lower house of parliament in 2018.\nGlobal Trends in Female Representation Conveniently, the dataset has a precalculated global aggregate statistic.\nwip %\u0026gt;% filter(country_name == \u0026quot;World\u0026quot;) %\u0026gt;% ggplot(aes(year, prop_women_seats)) + geom_point() + geom_line() + scale_x_continuous( breaks = seq(1998, 2018, by = 2), minor_breaks = FALSE ) + scale_y_continuous(labels = scales::percent_format(accuracy = 1, scale = 1)) + theme_minimal() + labs( title = \u0026quot;There has been a constant upward trend globally since 1997\u0026quot;, x = NULL, y = \u0026quot;Percentage of parliament seats held by women\u0026quot; ) It is encouraging to see that female representation has increased every year when we look at the whole world aggregated.\nBy region regions \u0026lt;- c( \u0026quot;East Asia \u0026amp; Pacific\u0026quot;, \u0026quot;Europe \u0026amp; Central Asia\u0026quot;, \u0026quot;Latin America \u0026amp; Caribbean\u0026quot;, \u0026quot;Middle East \u0026amp; North Africa\u0026quot;, \u0026quot;Sub-Saharan Africa\u0026quot;, \u0026quot;North America\u0026quot;, \u0026quot;South Asia\u0026quot; ) wip %\u0026gt;% filter(country_name %in% regions) %\u0026gt;% drop_na(prop_women_seats) %\u0026gt;% ggplot(aes(year, prop_women_seats, color = country_name)) + geom_line(size = 1) + geom_point(size = 1) + ggrepel::geom_text_repel( aes(label = country_name), data = . %\u0026gt;% group_by(country_name) %\u0026gt;% top_n(1, wt = year), nudge_x = 0.25, direction = \u0026quot;y\u0026quot;, hjust = 0 ) + scale_x_continuous( breaks = seq(1998, 2018, 2), minor_breaks = NULL, limits = c(1997, 2028) ) + scale_y_continuous(labels = scales::percent_format(accuracy = 1, scale = 1)) + guides(color = \u0026quot;none\u0026quot;) + theme_minimal() + labs( x = NULL, y = \u0026quot;Percentage of parliament seats held by women\u0026quot;, title = \u0026quot;All regions but South Asia have had increasing representation since 1997\u0026quot;, subtitle = \u0026quot;Latin America \u0026amp; Caribbean and Middle East \u0026amp; North Africa have seen large incerases\u0026quot; ) Notably, Latin America \u0026amp; the Caribbean went from the fourth leading region in terms of female representation in the late 1990s to the leading region by a relatively significant margin by 2014. The Middle East \u0026amp; North Africa, though still lagging behind the rest of the world, also has experienced a marked increase since 1997. Interestingly, South Asia is the only region that has shown a decrease, with representation plateauing in 2009, and then beginning to decrease.\n By income level Another interesting aggregation is by income level of the countries.\norder \u0026lt;- c( \u0026quot;High income\u0026quot;, \u0026quot;Upper middle income\u0026quot;, \u0026quot;Middle income\u0026quot;, \u0026quot;Lower middle income\u0026quot;, \u0026quot;Low income\u0026quot; ) wip %\u0026gt;% filter(country_name %in% order) %\u0026gt;% mutate(country_name = factor(country_name, levels = order, ordered = TRUE)) %\u0026gt;% drop_na(prop_women_seats) %\u0026gt;% ggplot(aes(year, prop_women_seats, color = country_name)) + geom_line(aes(group = country_name), color = \u0026quot;white\u0026quot;, size = 2) + geom_point(aes(group = country_name), color = \u0026quot;white\u0026quot;, size = 1) + geom_point(size = 1) + geom_line(size = 1) + ggrepel::geom_text_repel( aes(label = country_name), data = . %\u0026gt;% group_by(country_name) %\u0026gt;% top_n(1, wt = year), nudge_x = 0.25, direction = \u0026quot;y\u0026quot;, hjust = 0, color = \u0026quot;grey30\u0026quot; ) + scale_x_continuous( breaks = seq(1998, 2018, 2), minor_breaks = NULL, limits = c(1997, 2025) ) + scale_y_continuous(labels = scales::percent_format(accuracy = 1, scale = 1)) + guides(color = \u0026quot;none\u0026quot;) + theme_minimal() + labs( title = \u0026quot;Low income countries outperform lower middle income countries\u0026quot;, x = NULL, y = \u0026quot;Percentage of parliament seats held by women\u0026quot; ) Unsurprisingly, based on the previous two plots, representation is increasing over time at all income levels. There is something very striking about this plot, though. Representation has been higher with higher income, with the obvious exception of low income regions. Representation in low income regions has been higher than in lower middle income regions every year recorded, and in three different years has been second only to high income regions.\n By country It is really difficult to look at the trend of all nations at once, given how many there are. We can visualize the most recent data across the globe by looking at a map of the world with data from 2018.\nmidpoint \u0026lt;- wip %\u0026gt;% filter(year == 2018) %\u0026gt;% semi_join(world %\u0026gt;% select(country_code = iso_a3)) %\u0026gt;% summarize(median_prop_women = median(prop_women_seats, na.rm = TRUE)) %\u0026gt;% pull(median_prop_women) ## Joining, by = \u0026quot;country_code\u0026quot; world %\u0026gt;% left_join( wip %\u0026gt;% filter(year == 2018), by = c(\u0026quot;iso_a3\u0026quot; = \u0026quot;country_code\u0026quot;) ) %\u0026gt;% ggplot() + geom_sf( aes(fill = prop_women_seats), color = \u0026quot;grey20\u0026quot;, size = 0.1 ) + scale_fill_gradient( low = \u0026quot;white\u0026quot;, high = \u0026quot;#01665e\u0026quot;, labels = scales::percent_format(accuracy = 1, scale = 1) ) + coord_sf(datum = NA) + guides( fill = guide_colorbar( barwidth = 8, barheight = 0.5 ) ) + theme_void() + theme( legend.position = \u0026quot;top\u0026quot;, legend.title.align = 1 ) + labs(fill = \u0026quot;Proportion of seats\\nheld by women\u0026quot;)   Case study: Rwanda Though it’s hard to see from the map, at the very beginning we noticed that Rwanda had the highest female representation in its parliament in 2018, at over 60%! Looking into the politics there a bit more, I found that they introduced a quota system in 2003 that required 30% of the parliament to be made up of women. Looking at the data, there is a clear increase at the time this policy began.\nwip %\u0026gt;% drop_na(prop_women_seats) %\u0026gt;% filter(country_name == \u0026quot;Rwanda\u0026quot;) %\u0026gt;% ggplot(aes(year, prop_women_seats)) + geom_hline(yintercept = 50, size = 2, color = \u0026quot;grey80\u0026quot;) + geom_vline(xintercept = 2003, size = 1, color = \u0026quot;#b2182b\u0026quot;) + geom_line() + geom_point() + annotate( geom = \u0026quot;text\u0026quot;, x = 2003.25, y = 40, label = \u0026quot;30% quota\\nintroduced\\nin 2003\u0026quot;, hjust = 0, color = \u0026quot;grey40\u0026quot; ) + scale_x_continuous( breaks = seq(1998, 2018, 2), minor_breaks = NULL ) + scale_y_continuous(labels = scales::percent_format(accuracy = 1, scale = 1)) + theme_minimal() + labs( title = \u0026quot;Rwanda reached almost 50/50 representation after the introduction of a quota\u0026quot;, subtitle = \u0026quot;Representation continued to increase even 10 years later\u0026quot;, x = NULL, y = \u0026quot;Percentage of parliament seats held by women\u0026quot; )  Quota Systems The case study of Rwanda made me think more about quotas: how effective are they? where have they been implemented? Conveniently, the Institute for Democracy and Electoral Assistance (IDEA) has a public dataset of the types of quotas implemented all over the world! The only downside is that the database doesn’t include the year the quota was introduced. This is something that could be collated manually but we can work without for now.\nThere are three types of “quotas” in the dataset, explicit quotas about the number or fraction of seats to be held by women, a reserved seats system where certain seats must be held by women, and voluntary systems where some parties elect to hold themselves to a quota but none exists in law. We can visualize how these systems are implemented around the world with a map.\ntype_order \u0026lt;- quotas %\u0026gt;% count(quota_type, sort = TRUE) %\u0026gt;% pull(quota_type) world %\u0026gt;% left_join( quotas %\u0026gt;% mutate( quota_type = factor( quota_type, levels = type_order, ordered = TRUE ) ), by = c(\u0026quot;iso_a3\u0026quot; = \u0026quot;country_code\u0026quot;) ) %\u0026gt;% ggplot() + geom_sf(aes(fill = quota_type), color = \u0026quot;white\u0026quot;, size = 0.1) + scale_fill_brewer( type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;, na.value = \u0026quot;grey80\u0026quot; ) + theme_minimal() + theme(legend.position = \u0026quot;top\u0026quot;) + coord_sf(datum = NA) + labs(x = NULL, y = NULL, fill = \u0026quot;Quota Type\u0026quot;) We can see that South \u0026amp; Central America have almost completely adopted quotas. As we noticed above, this region has had particularly notable improvement in female representation since 1997.\nNow to make any causal claims about quotas and representation trends would need much more careful analysis, so I should emphasize I am not trying to do that. There are a couple of simple questions we could answer with this data, though.\nDo the countries with the biggest increase in representation from 1997 to 2018 also have quota systems in place?  differences \u0026lt;- wip_quotas %\u0026gt;% filter(year %in% c(1997, 2018)) %\u0026gt;% spread(key = year, value = prop_women_seats) %\u0026gt;% mutate( difference = `2018` - `1997`, quota_type = if_else(is.na(quota_type), \u0026quot;None\u0026quot;, quota_type) ) %\u0026gt;% drop_na(difference) differences %\u0026gt;% top_n(n = 10, wt = difference) %\u0026gt;% arrange(desc(difference)) %\u0026gt;% select(country_name, quota_type, difference) %\u0026gt;% knitr::kable(caption = \u0026quot;The countries with the biggest increase in percentage of parliament seats held by women from 1997 to 2018\u0026quot;)  Table 3: The countries with the biggest increase in percentage of parliament seats held by women from 1997 to 2018  country_name quota_type difference    Rwanda Reserved seats 44.2  Ethiopia Voluntary 36.8  North Macedonia Quota 35.0  Nicaragua Quota 34.9  Ecuador Quota 34.3  Mexico Quota 34.0  Cuba None 30.4  Senegal Quota 30.1  Costa Rica Quota 29.8  Nepal Reserved seats 29.3    In fact only two of the top 10 countries by increase 1997-2018 did not have some form of legally binding quota system in place.\nDid countries with quota systems in place see larger year-to-year jumps than those without?  largest_jumps \u0026lt;- wip_quotas %\u0026gt;% group_by(country_code) %\u0026gt;% arrange(year) %\u0026gt;% mutate( jump = lag(prop_women_seats) - prop_women_seats, quota_type = if_else(is.na(quota_type), \u0026quot;None\u0026quot;, quota_type) ) %\u0026gt;% drop_na(jump) %\u0026gt;% summarize( largest_jump = max(jump), quota_type = first(quota_type) ) %\u0026gt;% filter( quota_type != \u0026quot;Funding incentives\u0026quot; # only one country ) ## `summarise()` ungrouping output (override with `.groups` argument) median_largest_jump \u0026lt;- largest_jumps %\u0026gt;% summarize(med = median(largest_jump)) %\u0026gt;% pull(med) largest_jumps %\u0026gt;% ggplot(aes(quota_type, largest_jump)) + geom_hline(yintercept = median_largest_jump, size = 2, color = \u0026quot;white\u0026quot;) + geom_violin(draw_quantiles = 0.5) + scale_y_continuous(labels = scales::percent_format(accuracy = 1, scale = 1)) + labs( x = \u0026quot;Quota type\u0026quot;, y = \u0026quot;Largest one-year jump in female representation from 1997 to 2018\u0026quot;, title = \u0026quot;More countries with quota systems saw large year-to-year jumps\\nin female representation\u0026quot;, subtitle = \u0026quot;Though some very large jumps occurred in countries without quotas\u0026quot; ) To make this plot, I calculated the largest jump in percentage of seats held by women from one year to the next for each country. The distribution of these jumps by quota system is then plotted as a violin plot. The white reference line shows the median largest jump size across all countries. Each line within a violin plot shows the median largest jump size for countries with that type of quota system in place.\nTwo things are clear:\n More than 50% of countries with some kind of quota system experienced a larger jump than the median largest jump in countries with no quota system at all. Some countries with no quota system experienced extremely large jumps from one year to the next. Clearly quotas are not necessary to achieve this.  Side note: since I was curious, I looked up that enormous outlier. It’s Andorra and they had an election in 2011 where 15 women were elected making them the first country in Europe to have a majority female parliament.   Like I said before, I am being careful not to make any causal claims here. Clearly, representation can be increased with or without quotas. Exploring these datasets has been lots of fun, and there are definitely some very positive examples that give hope for a more equal future!\n ","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"ad91a88c6bb232a0fe658ecfd2985d09","permalink":"/post/2019-08-07-womeninparliament/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/2019-08-07-womeninparliament/","section":"post","summary":"This week’s RWeekly digest included an awesome dataset from the World Bank that documents the percentage of women in lower house parliament across the globe from 1997 to 2018. The dataset is nearly complete and also includes summary statistics for numerous geographic aggregations.\nAll of my code to process and explore the data is available on my GitHub. There’s lots of interesting things to look at in this data, so I will leave most of the boring stuff out of this post.","tags":["R","data-visualization","tidyverse"],"title":"Women in Parliament Worldwide","type":"post"},{"authors":null,"categories":[],"content":" It’s not often I see a cool data project focused on Australia, and in particular Australian politics, so I was especially excited to see freerangestats’ exploration of the 2016 Australian Election Study featured in RWeekly this week! The RWeekly email included this visualization, showing the responses to 6 questions about Australia’s parliamentary system, organized by party preference for Senate in the 2013 Federal Election.\nAt first, I was excited to see where voters for my party came in relative to other voters in parliamentary trivia, but then… how can I even compare? I quickly realized that I have many qualms with this plot.\nFirst, it takes some hunting to find the correct answers to each question (they’re listed in the subtitle), and once you’ve found them you have to go through six facets and match responses to answers. The plot shows the proportion of respondents who answered True, False or Don’t Know to each question. In my opinion, it would be far more intuitive to show only what we really care about: who was right!\nThen there’s the choice of the stacked bar chart. Firstly, placing the “Don’t Know” responses at the middle, centered at 0, makes it almost impossible to see the percentages of “True” and “False” respondents, about whom we likely care far more. Secondly, the choice to make “False” go in the “negative” (labelled positive) direction on the axis, as well as the color choice, makes me intuitively assume that the orange indicates incorrect answers and green indicates correct.\nFinally, what set me off in the first place: we can’t compare between parties! The orientation of the facets makes it very difficult to compare in the horizontal direction, even if the bars did all start at zero.\nHere’s my proposed improvement on the visualization from freerangestats:\nIt looks from this plot like the Greens voters have outdone everyone across the board, followed by voters for the coalition. We shouldn’t draw any conclusions too quickly though, keep reading to see the code and explore how education levels influence Australian voters’ knowledge of their parliament.\nImporting and Wrangling the Data # Libraries # ========= library(tidyverse) library(haven) # Constants # ========= FILE_DATA \u0026lt;- \u0026quot;~/Data/aus-election-poll/aes2016.sas7bdat\u0026quot; QUESTION_ABBRS \u0026lt;- c( \u0026quot;federation_1901\u0026quot;, \u0026quot;prop_rep\u0026quot;, \u0026quot;time_between_elections\u0026quot;, \u0026quot;constitution\u0026quot;, \u0026quot;deposit\u0026quot;, \u0026quot;num_members_hrp\u0026quot; ) QUESTION_LABELS \u0026lt;- c( \u0026quot;Australia became a\\nfederation in 1901\\n(True)\u0026quot;, \u0026quot;The Senate election is\\nbased on proportional representation\\n(True)\u0026quot;, \u0026quot;The longest time allowed between\\nFederal elections for the House\\nof Representatives is four years\\n(False)\u0026quot;, \u0026quot;The Constitution can only be\\nchanged by the High Court\\n(False)\u0026quot;, \u0026quot;No-one may stand for Federal\\nparliament unless they pay a deposit\\n(True)\u0026quot;, \u0026quot;There are 75 members of\\nthe House of Representatives\\n(False)\u0026quot; ) QUESTION_LABELS_SHORT \u0026lt;- c( \u0026quot;Federation\u0026quot;, \u0026quot;Proportional Representation\u0026quot;, \u0026quot;Years Between Elections\u0026quot;, \u0026quot;Changing the Constitution\u0026quot;, \u0026quot;Deposit to Stand\\nfor Federal Election\u0026quot;, \u0026quot;Number of Members\\nin the House of Reps\u0026quot; ) PARTY_LEVELS \u0026lt;- c( \u0026quot;Coalition\u0026quot;, \u0026quot;Labor Party (ALP)\u0026quot;, \u0026quot;Greens\u0026quot;, \u0026quot;Other (incl. no vote)\u0026quot; ) PARTY_COLORS \u0026lt;- c(\u0026quot;#1b4f9c\u0026quot;, \u0026quot;#e43340\u0026quot;, \u0026quot;#009c3d\u0026quot;, \u0026quot;grey60\u0026quot;) EDUCATION_LEVELS \u0026lt;- c( \u0026quot;some secondary\u0026quot;, \u0026quot;secondary\u0026quot;, \u0026quot;trade\u0026quot;, \u0026quot;university\u0026quot; ) EDUCATION_LABELS \u0026lt;- c( \u0026quot;Some secondary school\\nor less\u0026quot;, \u0026quot;Secondary school only\u0026quot;, \u0026quot;Trade Qualification\\nor other Diploma\u0026quot;, \u0026quot;Bachelor\u0026#39;s Degree\\nor higher\u0026quot; ) # Functions # ========= recode_truefalse \u0026lt;- function(x, answer) { recode( x, `1` = TRUE, `2` = FALSE, .default = NA ) %in% answer } recode_party \u0026lt;- function(x) { recode( x, `1` = \u0026quot;Coalition\u0026quot;, `2` = \u0026quot;Labor Party (ALP)\u0026quot;, `3` = \u0026quot;Coalition\u0026quot;, `4` = \u0026quot;Greens\u0026quot;, `997` = NA_character_, `998` = NA_character_, .default = \u0026quot;Other (incl. no vote)\u0026quot; ) } recode_01 \u0026lt;- function(x) { if_else(x %in% 997:999, NA_real_, x) } import_vars \u0026lt;- list( weight = list(var = \u0026quot;wt_enrol\u0026quot;, fun = identity), senate_vote = list(var = \u0026quot;B9_2\u0026quot;, fun = recode_party), schooling = list(var = \u0026quot;G1\u0026quot;, fun = recode_01), tertiary = list(var = \u0026quot;G3\u0026quot;, fun = recode_01), federation_1901 = list(var = \u0026quot;F10_1\u0026quot;, fun = function(.) recode_truefalse(., TRUE)), num_members_hrp = list(var = \u0026quot;F10_2\u0026quot;, fun = function(.) recode_truefalse(., FALSE)), constitution = list(var = \u0026quot;F10_3\u0026quot;, fun = function(.) recode_truefalse(., FALSE)), prop_rep = list(var = \u0026quot;F10_4\u0026quot;, fun = function(.) recode_truefalse(., TRUE)), deposit = list(var = \u0026quot;F10_5\u0026quot;, fun = function(.) recode_truefalse(., TRUE)), time_between_elections = list(var = \u0026quot;F10_6\u0026quot;, fun = function(.) recode_truefalse(., FALSE)) ) The first step is to read in and wrangle the data. I downloaded the data in SAS format, so I use haven::read_sas to read the raw data. The list import_vars specifies the columns we would like to keep, the column names we would like to assign, and the functions we would like to apply to recode the survey response variables to a readable format.\nThere are many encodings in the survey data that should be recoded. For example, AES encodes “Item skipped” and “Does not apply”, responses we would like to encode as NA, as numbers 997 and 999. I also define functions to redefine the “True”/“False”/“Don’t Know” responses into logical vectors representing whether the respondent answered correctly, and to give names to the political parties. I chose to collapse the Liberal Party and the National (Country) Party into the Coalition, and drop One Nation to make the visualizations easier to digest.\nI define my own education variable based on responses to two questions: “How old were you when you left secondary school?” and “Have you obtained a trade qualification, a degree or a diploma, or any other qualification since leaving school?”\ndf \u0026lt;- read_sas(data_file = FILE_DATA) true_false \u0026lt;- import_vars %\u0026gt;% map_dfc(~ .$fun(pull(df, .$var))) %\u0026gt;% mutate( education = case_when( schooling \u0026gt; 1 ~ \u0026quot;some secondary\u0026quot;, tertiary == 1 ~ \u0026quot;secondary\u0026quot;, tertiary %in% c(2, 3) ~ \u0026quot;university\u0026quot;, tertiary %in% c(4, 5, 6, 7) ~ \u0026quot;trade\u0026quot;, TRUE ~ NA_character_ ), education = factor( education, levels = EDUCATION_LEVELS, labels = EDUCATION_LABELS, ordered = TRUE ), senate_vote = factor(senate_vote, levels = PARTY_LEVELS) ) %\u0026gt;% select(-schooling, -tertiary)  Visualizing Responses by Party Now that the data has been cleaned up, I can create my version of the freerangestats visualization.\nresponses \u0026lt;- true_false %\u0026gt;% drop_na() %\u0026gt;% select(-weight, -education) %\u0026gt;% group_by(senate_vote) %\u0026gt;% summarise_all(mean, na.rm = TRUE) %\u0026gt;% gather( key = \u0026quot;question\u0026quot;, value = \u0026quot;prop_correct\u0026quot;, -senate_vote ) %\u0026gt;% mutate( question = factor( question, levels = QUESTION_ABBRS, labels = QUESTION_LABELS ) ) responses %\u0026gt;% ggplot( aes( reorder(question, -prop_correct), prop_correct, color = senate_vote ) ) + geom_hline(yintercept = 0.5, color = \u0026quot;white\u0026quot;, size = 2) + geom_line(aes(group = senate_vote)) + geom_point() + scale_color_manual( breaks = PARTY_LEVELS, values = PARTY_COLORS ) + scale_y_continuous( breaks = seq(0, 1, by = 0.1), labels = scales::percent_format(accuracy = 1) ) + theme( axis.text.x = element_text(size = 7), legend.position = \u0026quot;top\u0026quot;, legend.title = element_blank() ) + labs( title = \u0026quot;Australians\u0026#39; Knowledge of Constitutional Issues\u0026quot;, subtitle = \u0026quot;By Senate Vote in the 2013 Federal Election\u0026quot;, caption = \u0026quot;Source: Australian Election Study 2016\u0026quot;, x = \u0026quot;True/False Question (Answer)\u0026quot;, y = \u0026quot;Percentage of respondents who answered correctly\u0026quot; ) I think this plot is an improvement for a few reasons:\n We can very easily compare between parties, and can very quickly identify parties by their official colors\n We can quickly see the percentage of correct respondents\n We can see which questions were easier and which were more difficult for the respondents\n The answers are listed with the questions\n  There is some information lost, for example we can no longer see how many people said they didn’t know the answer, and it is much more difficult to figure out the raw responses. However, in my opinion this is far less important information in this context.\n Controlling for Education Level The AES gives us lots of details about the respondents including their education level. Although the plot above appears to indicate that Greens voters know the most about the parliamentary system, I was wary to jump to this conclusion without controlling for the level of education of the voters.\nTo make inferences about the enrolled population, we need to use the provided weights. Then we are able to calculate the proportion of the electorate and the proportion of voters for each of the Coalition, ALP and the Greens at each level of education.\ntotal_ed_props \u0026lt;- true_false %\u0026gt;% drop_na() %\u0026gt;% count(education, wt = weight) %\u0026gt;% mutate(prop_educated = n / sum(n), group = \u0026quot;All respondents\u0026quot;) ed_props_by_party \u0026lt;- true_false %\u0026gt;% drop_na() %\u0026gt;% filter(senate_vote != \u0026quot;Other (incl. no vote)\u0026quot;) %\u0026gt;% count(senate_vote, education, wt = weight) %\u0026gt;% group_by(senate_vote) %\u0026gt;% mutate(prop_educated = n / sum(n)) %\u0026gt;% ungroup() Plotting these proportions shows dramatic disparities:\ned_props_by_party %\u0026gt;% ggplot(aes(education, prop_educated)) + geom_line( aes(group = group), color = \u0026quot;grey60\u0026quot;, data = total_ed_props ) + geom_point(color = \u0026quot;grey60\u0026quot;, data = total_ed_props) + geom_line(aes(group = senate_vote, color = senate_vote)) + geom_point(aes(color = senate_vote)) + scale_y_continuous( breaks = seq(0, 0.5, by = 0.1), labels = scales::percent_format(accuracy = 1) ) + scale_color_manual( breaks = PARTY_LEVELS, values = PARTY_COLORS ) + theme( legend.position = \u0026quot;top\u0026quot;, axis.text.x = element_text(size = 8) ) + labs( title = \u0026quot;The highly educated are far overrepresented amongst Greens voters\u0026quot;, subtitle = \u0026quot;Over 55% of Greens voters are university graduates, compared to 37% of the\\n enrolled population (shown in grey)\u0026quot;, x = NULL, y = \u0026quot;Percentage of voters by party\\nweighted to match enrolled population\u0026quot;, color = NULL, caption = \u0026quot;Source: Australian Election Study 2016\u0026quot; ) As we see in the plot above, university graduates are far overrepresented amongst Greens voters, while voters for Labor and the Coalition are reflective of national trends in education levels.\nTo understand whether educational differences account for the variation in performance on the True/False questions, we need to do some further exploration. To simplify this, I decided to calculate a score for each respondent. A simple and interpretable score is the proportion of correct answers to the 6 True/False questions.\nscores \u0026lt;- true_false %\u0026gt;% drop_na() %\u0026gt;% mutate( frac_correct = (federation_1901 + prop_rep + time_between_elections + constitution + deposit + num_members_hrp) / 6 ) We can then visualize the distribution of scores at each education level.\nscores %\u0026gt;% ggplot(aes(education, frac_correct)) + geom_hline(yintercept = 0.5, size = 2, color = \u0026quot;white\u0026quot;) + geom_boxplot(varwidth = TRUE) + scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + labs( x = \u0026quot;Highest Level of Education Attained\u0026quot;, y = \u0026quot;Percentage of questions answered correctly (of 6 total)\u0026quot;, title = \u0026quot;Median Performance Increases with Education Level\u0026quot;, subtitle = \u0026quot;But performance varies widely at all levels\u0026quot;, caption = \u0026quot;Source: Australian Election Study 2016\u0026quot; ) Despite the wide variation at every level of education, it does appear that university graduates performed the best on the multiple choice questions, with the top half of respondents with a Bachelor’s degree or higher getting at least half of the questions correct.\nFinally, we can visualize the joint distribution of performance across the six questions, across the levels of education and across the party preferences. I chose to leave out those with less than a secondary school education, as this was a very small sample.\nresponses_with_ed \u0026lt;- true_false %\u0026gt;% drop_na() %\u0026gt;% select(-weight) %\u0026gt;% filter(education != \u0026quot;Some secondary school\\nor less\u0026quot;) %\u0026gt;% group_by(senate_vote, education) %\u0026gt;% summarise_all(mean, na.rm = TRUE) %\u0026gt;% gather( key = \u0026quot;question\u0026quot;, value = \u0026quot;prop_correct\u0026quot;, -senate_vote, -education ) %\u0026gt;% mutate( question = factor( question, levels = QUESTION_ABBRS, labels = QUESTION_LABELS_SHORT ) ) responses_with_ed %\u0026gt;% ggplot( aes( reorder(question, -prop_correct), prop_correct, color = senate_vote ) ) + geom_hline(yintercept = 0.5, color = \u0026quot;white\u0026quot;, size = 2) + geom_line(aes(group = senate_vote)) + geom_point() + scale_color_manual( breaks = PARTY_LEVELS, values = PARTY_COLORS ) + scale_y_continuous( breaks = seq(0, 1, by = 0.1), labels = scales::percent_format(accuracy = 1) ) + theme( axis.text.x = element_text(size = 9, angle = 45, hjust = 1), legend.position = \u0026quot;top\u0026quot;, legend.title = element_blank() ) + labs( title = \u0026quot;Australians\u0026#39; Knowledge of Constitutional Issues by 2013 Senate Vote and Education Level\u0026quot;, subtitle = \u0026quot;University graduates outperform those less educated regardless of party affiliation\u0026quot;, caption = \u0026quot;Source: Australian Election Study 2016\u0026quot;, x = \u0026quot;Question Topic\u0026quot;, y = \u0026quot;Percentage of respondents who answered correctly\u0026quot; ) + facet_grid(cols = vars(education)) As we might’ve expected, the gap between voters across party preferences closes entirely with education level. Though it appears that accuracy amongst those with only a secondary school education does vary with party affiliation, this variation is much less pronounced amongst those with trade or other non-degree qualifications and completely indistinguishable amongst those with a university degree.\nI will certainly be continuing to explore this data. I’m excited to think more about why the variation remains amongst secondary school graduates, or perhaps how socioeconomic status comes into play. It would also be interesting to break down the Coalition into Liberal voters and National Party voters. I’ll also be looking into some longitudinal analysis with the AES data!\n ","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"b5ecf25db2e47ec5375e2582b423ba52","permalink":"/post/2019-04-29-auselectionpoll/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/post/2019-04-29-auselectionpoll/","section":"post","summary":"It’s not often I see a cool data project focused on Australia, and in particular Australian politics, so I was especially excited to see freerangestats’ exploration of the 2016 Australian Election Study featured in RWeekly this week! The RWeekly email included this visualization, showing the responses to 6 questions about Australia’s parliamentary system, organized by party preference for Senate in the 2013 Federal Election.\nAt first, I was excited to see where voters for my party came in relative to other voters in parliamentary trivia, but then… how can I even compare?","tags":["R","data-visualization","tidyverse","improve-this-vis"],"title":"How Much Do Australians Know About Their Parliament?","type":"post"},{"authors":null,"categories":[],"content":" This month I decided to take part in the Storytelling With Data monthly challenge for the first time! The dataset we were given to explore contains global aid exchanges between 47 countries across the world across the years 1973-2013. The goal is to create visualizations that answers the broad question: Who Donates?, as well as some bonus questions about distribution of donations geographically, temporally and by purpose of donation. Here’s my initial attempt! Along with some code. (The only package I use here is tidyverse).\nThe Data The data provided is nice and clean, so all we are left to do is read it in using read_csv(). I changed some variable names to make it nicer to work with, and noticed that there are a few negative quantities of money in the data, which I drop since they are impossible. Here’s a glimpse of what the data looks like:\n    id year donor recipient amount purpose_code purpose_desc    2414478 1977 Saudi Arabia India 348718518 23030 Power generation/renewable sources  2414509 1977 Saudi Arabia Brazil 191647004 23040 Electrical transmission/ distribution  2414635 1983 Saudi Arabia India 79371799 21030 Rail transport  2414665 1984 Saudi Arabia Taiwan 212202942 21030 Rail transport  2414667 1984 Saudi Arabia Korea 134511154 21040 Water transport  2414684 1985 Saudi Arabia India 128074768 23000 Energy generation and supply, combinations of activities     Who Donates? One of the challenges in answering this question is how to summarize across time. I chose to look at the proportion of the total money contributed to global aid that each country contributed and received.\ndonated \u0026lt;- data %\u0026gt;% group_by(donor) %\u0026gt;% summarise(donated = sum(amount)) %\u0026gt;% mutate(prop_donated = donated / sum(donated)) %\u0026gt;% select(country = donor, prop_donated) ## `summarise()` ungrouping output (override with `.groups` argument) received \u0026lt;- data %\u0026gt;% group_by(recipient) %\u0026gt;% summarise(received = sum(amount)) %\u0026gt;% mutate(prop_received = received / sum(received)) %\u0026gt;% select(country = recipient, prop_received) ## `summarise()` ungrouping output (override with `.groups` argument) aid \u0026lt;- donated %\u0026gt;% full_join(received, by = c(\u0026quot;country\u0026quot;)) %\u0026gt;% mutate_at(vars(-country), ~if_else(is.na(.), 0, .)) %\u0026gt;% gather(-country, key = direction, value = proportion_of_aid) %\u0026gt;% mutate(direction = str_extract(direction, \u0026quot;[^_]+$\u0026quot;)) country_order \u0026lt;- aid %\u0026gt;% spread(direction, proportion_of_aid) %\u0026gt;% mutate(diff = donated - received) %\u0026gt;% arrange(diff) %\u0026gt;% pull(country) aid \u0026lt;- aid %\u0026gt;% mutate(country = factor(country, levels = country_order, ordered = TRUE)) segments \u0026lt;- aid %\u0026gt;% spread(direction, proportion_of_aid) aid %\u0026gt;% ggplot(aes(y = country)) + geom_segment( aes(yend = country, x = donated, xend = received), color = \u0026quot;grey40\u0026quot;, data = segments ) + geom_point(aes(x = proportion_of_aid, color = direction), size = 2) + scale_y_discrete(expand = expand_scale(0)) + scale_x_sqrt( labels = scales::percent, expand = expand_scale(0), limits = c(0,0.4), breaks = c(0, 0.01, 0.025, 0.05, 0.1, 0.2, 0.3, 0.4) ) + scale_color_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Set1\u0026quot;, labels = str_to_title) + theme_minimal() + theme(legend.position = \u0026quot;top\u0026quot;) + coord_cartesian(clip = \u0026quot;off\u0026quot;) + labs( y = NULL, x = glue::glue(\u0026quot;Percentage of Total Aid {min(pull(data, year))} - {max(pull(data, year))}\u0026quot;), color = NULL, title = \u0026quot;The United States and Japan are the world\u0026#39;s major donors\u0026quot;, subtitle = \u0026quot;India has received almost 40% of all global aid\u0026quot; ) ## Warning: `expand_scale()` is deprecated; use `expansion()` instead. ## Warning: `expand_scale()` is deprecated; use `expansion()` instead. Excuse the squished y-axis. I played around with it for a while and eventually gave up. Any hints are very welcome!\n Has the Amount Donated Changed Over Time? In keeping with the same metric, proportion of aid contributed and received, we can also look at the trends over time. I’ve highlighted the top three donors and recipients in the figure. Interestingly, it seems that receiving tends to be steadier over time, while donations see more anomalous spikes\ndonated \u0026lt;- data %\u0026gt;% group_by(donor, year) %\u0026gt;% summarise(donated = sum(amount)) %\u0026gt;% ungroup() %\u0026gt;% mutate(prop_donated = donated / sum(donated)) %\u0026gt;% select(country = donor, year, prop_donated) ## `summarise()` regrouping output by \u0026#39;donor\u0026#39; (override with `.groups` argument) received \u0026lt;- data %\u0026gt;% group_by(recipient, year) %\u0026gt;% summarise(received = sum(amount)) %\u0026gt;% ungroup() %\u0026gt;% mutate(prop_received = received / sum(received)) %\u0026gt;% select(country = recipient, year, prop_received) ## `summarise()` regrouping output by \u0026#39;recipient\u0026#39; (override with `.groups` argument) timeseries \u0026lt;- donated %\u0026gt;% full_join(received, by = c(\u0026quot;country\u0026quot;, \u0026quot;year\u0026quot;)) %\u0026gt;% mutate_at(vars(prop_donated, prop_received), ~if_else(is.na(.), 0, .)) %\u0026gt;% gather(prop_donated, prop_received, key = direction, value = proportion) %\u0026gt;% mutate( proportion = if_else(direction == \u0026quot;prop_donated\u0026quot;, proportion, -proportion) ) top_3 \u0026lt;- aid %\u0026gt;% filter(direction == \u0026quot;donated\u0026quot;) %\u0026gt;% top_n(3, proportion_of_aid) %\u0026gt;% pull(country) bottom_3 \u0026lt;- aid %\u0026gt;% filter(direction == \u0026quot;received\u0026quot;) %\u0026gt;% top_n(3, proportion_of_aid) %\u0026gt;% pull(country) timeseries_main \u0026lt;- timeseries %\u0026gt;% filter(country %in% top_3 \u0026amp; direction == \u0026quot;prop_donated\u0026quot; | country %in% bottom_3 \u0026amp; direction == \u0026quot;prop_received\u0026quot;) country_order \u0026lt;- timeseries_main %\u0026gt;% filter(year == max(pull(data, year))) %\u0026gt;% arrange(desc(proportion)) %\u0026gt;% pull(country) timeseries_main \u0026lt;- timeseries_main %\u0026gt;% mutate(country = factor(country, levels = country_order, ordered = TRUE)) labeller \u0026lt;- function(y) { y = if_else(y \u0026lt; 0, -y, y) scales::percent(y) } timeseries %\u0026gt;% unite(group, country, direction, remove = FALSE) %\u0026gt;% ggplot(aes(year, proportion)) + geom_line(aes(group = group), alpha = 0.2) + geom_line(aes(color = country), data = timeseries_main) + scale_x_continuous(breaks = seq(1970, 2015, by = 5)) + scale_y_continuous(labels = labeller) + scale_color_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Dark2\u0026quot;) + theme_minimal() + labs( x = NULL, color = NULL, y = glue::glue(\u0026quot;Percentage of Annual Aid\u0026quot;), title = \u0026quot;Contributions from significant donors is not constant over time\u0026quot;, subtitle = \u0026quot;Major events like war and recession drive spikes in aid\u0026quot; ) + annotate( geom = \u0026quot;text\u0026quot;, x = 1974, y = 0.0275, label = \u0026quot;Proportion Donated\u0026quot;, hjust = 0 ) + annotate( geom = \u0026quot;text\u0026quot;, x = 1974, y = -0.0175, label = \u0026quot;Proportion Received\u0026quot;, hjust = 0 )  ","date":1552521600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552521600,"objectID":"4aa964349d629f5099e2c2a3c94a0dea","permalink":"/post/2019-03-07-swdchallenge/","publishdate":"2019-03-14T00:00:00Z","relpermalink":"/post/2019-03-07-swdchallenge/","section":"post","summary":"This month I decided to take part in the Storytelling With Data monthly challenge for the first time! The dataset we were given to explore contains global aid exchanges between 47 countries across the world across the years 1973-2013. The goal is to create visualizations that answers the broad question: Who Donates?, as well as some bonus questions about distribution of donations geographically, temporally and by purpose of donation. Here’s my initial attempt!","tags":["R","data-visualization","tidyverse"],"title":"#SWDChallenge March 2019","type":"post"},{"authors":null,"categories":[],"content":"    As my senior year at Stanford nears the end, I’ve started to think more and more about what I’ve really learned here, and what I’ll be taking away from my undergraduate experience. Sure, there’s plenty of sappy stuff about coming of age and figuring out what I want to start doing with my life (spoiler: haven’t figured it out), but what about concrete knowledge I’ve gained from my classes. Being the data nerd that I am, I decided to take a “data-driven” approach to this question, and see what I could find. Here’s the results!\nlibrary(tidyverse) library(googlesheets) library(rvest) library(tidytext) library(wordcloud) library(tm) library(igraph) library(networkD3) FILE.googlesheets \u0026lt;- \u0026quot;~/Data/course-descriptions/myclasses.Rds\u0026quot; FILE.explorecourses \u0026lt;- \u0026quot;~/Data/course-descriptions/courses1819raw.Rds\u0026quot; Collating the data First, we need some data! Although I played around with trying to scrape my transcript, it proved not worth the effort with all the Stanford security, so I created a Google Sheets document and manually recorded each class I have taken, including the department code e.g. “CS”, the course code e.g. “106A”, the school e.g. “School of Engineering”, the number of units I took the class for, and the quarter I took the class (recorded as an integer from 1-12).\nThen I was able to use the googlesheets package to pull this data into R with only a few lines of code:\n# Log in to Google gs_auth(new_user = TRUE) # Find the document we want sheet_key \u0026lt;- gs_ls() %\u0026gt;% filter(sheet_title == \u0026quot;courses\u0026quot;) %\u0026gt;% pull(sheet_key) # Read the document df \u0026lt;- gs_key(sheet_key) %\u0026gt;% gs_read() To avoid authentication over and over, I saved the resulting dataframe, and we’ll use the saved version.\nclasses \u0026lt;- read_rds(FILE.googlesheets) head(classes) %\u0026gt;% knitr::kable()   dept code school units quarter    esf 6a Office of Vice Provost for Undergraduate Education 7 1  lawgen 116n Law School 3 1  math 51 School of Humanities \u0026amp; Sciences 5 1  cs 106a School of engineering 5 2  physics 41 School of Humanities \u0026amp; Sciences 4 2  physics 41a School of Humanities \u0026amp; Sciences 1 2    Let’s clean up the formatting a little first. We’ll also add a variable that contains just the numeric part of the course code, in case we want to look at how advanced the coursework is by the course number.\nclasses \u0026lt;- classes %\u0026gt;% mutate( dept = str_to_upper(dept), code = str_to_upper(code), school = str_to_title(school), code_numeric = parse_number(code) ) head(classes) %\u0026gt;% knitr::kable()   dept code school units quarter code_numeric    ESF 6A Office Of Vice Provost For Undergraduate Education 7 1 6  LAWGEN 116N Law School 3 1 116  MATH 51 School Of Humanities \u0026amp; Sciences 5 1 51  CS 106A School Of Engineering 5 2 106  PHYSICS 41 School Of Humanities \u0026amp; Sciences 4 2 41  PHYSICS 41A School Of Humanities \u0026amp; Sciences 1 2 41    Just with this data we can already do some cool analysis. For example, let’s look at the distribution of units over the course of my degree, divided by school…\ntotal_unit_count \u0026lt;- classes %\u0026gt;% group_by(quarter) %\u0026gt;% summarise(total_units = sum(units)) ## `summarise()` ungrouping output (override with `.groups` argument) fall_quarters \u0026lt;- tribble( ~quarter, ~label, 1, \u0026quot;2015-2016\u0026quot;, 4, \u0026quot;2016-2017\u0026quot;, 7, \u0026quot;2017-2018\u0026quot;, 10, \u0026quot;2018-2019\u0026quot; ) classes %\u0026gt;% group_by(school, quarter) %\u0026gt;% summarise(total_units = sum(units)) %\u0026gt;% ggplot(aes(quarter, total_units)) + geom_vline(aes(xintercept = quarter), data = fall_quarters, color = \u0026quot;grey90\u0026quot;) + geom_point(aes(group = school, color = school)) + geom_line(aes(group = school, color = school)) + geom_line(data = total_unit_count, color = \u0026quot;grey80\u0026quot;) + geom_text( aes(x = quarter + 0.2, label = label), y = 14, color = \u0026quot;grey40\u0026quot;, angle = 90, data = fall_quarters, hjust = 0 ) + theme( legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;vertical\u0026quot;, axis.ticks.x = element_blank(), axis.text.x = element_blank() ) + labs( x = \u0026quot;Quarter\u0026quot;, y = \u0026quot;Units\u0026quot;, color = NULL, title = \u0026quot;Over Time I\u0026#39;ve started to take more classes in Engineering\\nand fewer in Humanities and Sciences\u0026quot; ) ## `summarise()` regrouping output by \u0026#39;school\u0026#39; (override with `.groups` argument) …what about by Department?\nclasses %\u0026gt;% group_by(school, dept) %\u0026gt;% summarise(total_units = sum(units)) %\u0026gt;% ggplot(aes(reorder(dept, total_units), total_units)) + geom_segment(aes(xend = dept, y = 0, yend = total_units), color = \u0026quot;grey60\u0026quot;) + geom_point(aes(color = school), size = 3) + scale_y_continuous(limits = c(0, 42), expand = c(0,0)) + theme_minimal() + theme( legend.position = \u0026quot;bottom\u0026quot;, legend.direction = \u0026quot;vertical\u0026quot; ) + coord_flip() + labs( x = NULL, y = \u0026quot;Total Units\u0026quot;, color = NULL, title = \u0026quot;Computer Science Dominates My Unit Count\u0026quot;, subtitle = \u0026quot;Math, MS\u0026amp;E and Statistics classes are also required of my major\u0026quot; ) ## `summarise()` regrouping output by \u0026#39;school\u0026#39; (override with `.groups` argument)  Scraping Course Descriptions What I am really curious about, though, is whether we can see any themes in my interests from the course descriptions of the classes I’ve taken, as listed in Explore Courses. I wasn’t about to copy and paste all of these by hand, plus this was a great opportunity to practice web scraping with rvest!\nI scraped ALL of the course descriptions from every page of Explore Courses, so that I can join the descriptions to my dataframe of classes. This took an hour or so to run, but I made sure to save the result!\nexplore_courses \u0026lt;- read_rds(FILE.explorecourses) head(explore_courses) ## # A tibble: 6 x 4 ## num title desc attrs ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 AA 47… Why Go To Space? \u0026quot;Why do we spend billions… \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… ## 2 AA 93: Building Trust i… \u0026quot;Preparatory course for B… \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… ## 3 AA 10… Introduction to … \u0026quot;This class introduces th… \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… ## 4 AA 10… Introduction to … \u0026quot;This course explores the… \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… ## 5 AA 10… Air and Space Pr… \u0026quot;This course is designed … \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… ## 6 AA 10… Surviving Space \u0026quot;Space is dangerous. Anyt… \u0026quot;\\r\\n\\t\\t\\t\\t\\t\\r\\n\\t\\t\\t… Clearly, the results are a little messy! For now, though all we want is the description to join to the datafram of my classes, and maybe the title of the classes might be fun too. To join to our dataset we need to split the num variable into department and code. While we’re at it, let’s drop any prerequisites from the desc variable. We don’t care about them for analysing the descriptions.\nexplore_courses \u0026lt;- explore_courses %\u0026gt;% mutate(num = str_extract(num, \u0026quot;^[^:]*\u0026quot;)) %\u0026gt;% separate(num, into = c(\u0026quot;dept\u0026quot;, \u0026quot;code\u0026quot;), sep = \u0026quot; \u0026quot;, extra = \u0026quot;merge\u0026quot;) %\u0026gt;% mutate(dept = str_to_upper(dept), code = str_to_upper(code)) %\u0026gt;% separate( desc, into = c(\u0026quot;desc\u0026quot;), sep = \u0026quot;(Prerequisites:|Prerequisite:|prerequisite:|prerequisites:)\u0026quot;, extra = \u0026quot;drop\u0026quot; ) %\u0026gt;% select(dept, code, title, desc) head(explore_courses, 2) %\u0026gt;% knitr::kable()     dept code title desc    AA 47SI Why Go To Space? Why do we spend billions of dollars exploring space? What can modern policymakers, entrepreneurs, and industrialists do to help us achieve our goals beyond planet Earth? Whether it is the object of exploration, science, civilization, or conquest, few domains have captured the imagination of a species like space. This course is an introduction to space policy issues, with an emphasis on the modern United States. We will present a historical overview of space programs from all around the world, and then spend the last five weeks discussing present policy issues, through lectures and guest speakers from NASA, the Department of Defense, new and legacy space industry companies, and more. Students will present on one issue that piques their interest, selecting from various domains including commercial concerns, military questions, and geopolitical considerations.  AA 93 Building Trust in Autonomy Preparatory course for Bing Overseas Studies summer course in Edinburgh.    That looks better!!\nNow we can join this to classes:\nclasses_with_descriptions \u0026lt;- classes %\u0026gt;% left_join(explore_courses, by = c(\u0026quot;dept\u0026quot;, \u0026quot;code\u0026quot;)) Even in the first few results, there are some NA values resulting from the join. A concern is that the matching of dept and code isn’t working due to a formatting quirk. Sadly, it’s even worse… Explore Courses deletes classes after they haven’t been offered for a while!\nclasses_with_descriptions %\u0026gt;% filter(is.na(desc)) %\u0026gt;% knitr::kable()   dept code school units quarter code_numeric title desc    LAWGEN 116N Law School 3 1 116 NA NA  PHYSICS 41A School Of Humanities \u0026amp; Sciences 1 2 41 NA NA  THINK 52 Office Of Vice Provost For Undergraduate Education 4 2 52 NA NA  CS 41 School Of Engineering 2 6 41 NA NA  CS 193X School Of Engineering 3 6 193 NA NA  ECON 5 School Of Humanities \u0026amp; Sciences 1 11 5 NA NA    Most of these are classes I took a while ago, and most are for few units, so excluding these from analysis of descriptions shouldn’t be a big deal!\n Analysing Course Descriptions Now we get to the fun part! The code in this section is adapted from this awesome blog post by Juan Orduz analyzing tweet data.\nLet’s start with something easy and classic, a wordcloud! For this we can use the packages tidytext to help with counting words, and wordcloud for visualization.\nclasses_with_descriptions %\u0026gt;% unnest_tokens(word, desc) %\u0026gt;% anti_join(get_stopwords()) %\u0026gt;% mutate(word = str_extract(word, \u0026quot;[:alpha:]+\u0026quot;)) %\u0026gt;% count(word) %\u0026gt;% arrange(desc(n)) %\u0026gt;% with( wordcloud( word, n, min.freq = 5, random.order = FALSE, colors = brewer.pal(8, \u0026#39;Dark2\u0026#39;) ) ) ## Joining, by = \u0026quot;word\u0026quot; No surprises here! But it’s still fun to see my degree described in pretty colored words.\nWhat is perhaps most interesting to me is whether we can see relationships between topics in the course descriptions! For this, I used the igraph package to create a graph, and the networkD3 package for visualization with D3.\nFirst we use tidytext to count the bigrams, the pairs of words that occur next to each other, in the descriptions.\nword_bigrams \u0026lt;- classes_with_descriptions %\u0026gt;% unnest_tokens( input = desc, output = bigram, token = \u0026#39;ngrams\u0026#39;, n = 2 ) %\u0026gt;% filter(!is.na(bigram)) %\u0026gt;% separate(col = bigram, into = c(\u0026#39;word1\u0026#39;, \u0026#39;word2\u0026#39;), sep = \u0026#39; \u0026#39;) %\u0026gt;% select(word1, word2) bigram_count \u0026lt;- word_bigrams %\u0026gt;% count(word1, word2, sort = TRUE) Now we can create a graph object and use the base R (gasp) plot function to visualize the relationships between words. We use a threshold to include only bigrams that occur more times than the threshold. It turns out that 2 is a good threshold for this dataset.\nthreshold \u0026lt;- 2 network \u0026lt;- bigram_count %\u0026gt;% filter(n \u0026gt; threshold) %\u0026gt;% graph_from_data_frame(directed = FALSE) plot( network, vertex.size = 1, vertex.label.color = \u0026#39;black\u0026#39;, vertex.label.cex = 0.7, vertex.label.dist = 1, edge.color = \u0026#39;gray\u0026#39;, main = \u0026#39;Course Descriptions\u0026#39;, alpha = 50 ) Even restricting the graph to bigrams that occur at least 3 times, it’s still hard to really see the relationships here. Now’s when we use D3 to take this visualization to the next level. Since there’s so much more interaction possible with a D3 graph, we’ll lower the threshold to see a little more detail.\nthreshold \u0026lt;- 1 network \u0026lt;- bigram_count %\u0026gt;% filter(n \u0026gt; threshold) %\u0026gt;% graph_from_data_frame(directed = FALSE) network_D3 \u0026lt;- igraph_to_networkD3(network) network_D3$nodes \u0026lt;- network_D3$nodes %\u0026gt;% mutate( degree = 10 * degree(network), group = 1 ) network_D3$links \u0026lt;- network_D3$links %\u0026gt;% mutate(width = 10 * E(network)$n / max(E(network)$n)) forceNetwork( Links = network_D3$links, Nodes = network_D3$nodes, Source = \u0026#39;source\u0026#39;, Target = \u0026#39;target\u0026#39;, NodeID = \u0026#39;name\u0026#39;, Group = \u0026#39;group\u0026#39;, opacity = 0.9, Value = \u0026#39;width\u0026#39;, Nodesize = \u0026#39;degree\u0026#39;, linkWidth = JS(\u0026quot;function(d) { return Math.sqrt(d.value); }\u0026quot;), fontSize = 12, zoom = TRUE, opacityNoHover = 1 )  {\"x\":{\"links\":{\"source\":[58,81,2,5,25,11,36,2,10,0,3,8,17,36,20,0,20,1,20,36,28,21,2,38,13,6,11,50,84,7,33,36,28,67,1,29,1,1,102,2,2,1,36,4,7,5,12,1,2,1,0,2,35,106,11,2,14,28,28,1,5,1,1,54,83,36,1,11,5,48,0,57,36,1,36,19,2,20,2,1,1,36,1,16,36,12,4,46,2,11,66,20,1,33,1,1,108,20,7,32,2,0,2,7,20,1,2,91,2,20,115,1,110,38,36,26,26,1,30,1,89,2,1,8,5,60,90,0,111,82,72,48,2,36,20,18,66,98,13,112,1,2,2,2,1,36,2,17,14,55,36,1,2,0,2,18,95,20,77,23,17,2,20,31,15,7,38,1,10,51,25,28,2,26,20,23,11,18,15,0,5,1,93,2,20,39,0,1,11,3,7,93,61,34,98,51,17,7,5,38,13,40,42,115,93],\"target\":[133,141,38,38,38,38,39,22,118,138,13,13,40,41,41,42,42,42,43,44,44,44,45,47,47,47,14,117,117,117,49,49,50,136,145,122,51,146,146,24,24,52,52,15,15,15,16,147,17,17,53,53,159,157,150,55,56,121,137,59,8,18,18,132,132,60,25,25,25,61,62,63,63,64,64,65,27,28,28,68,29,69,69,70,70,70,71,126,72,73,73,30,74,75,75,9,158,31,31,76,76,78,78,79,80,10,10,152,32,164,166,82,160,124,85,86,135,87,87,148,143,88,149,11,11,91,144,139,162,142,92,92,119,93,94,134,95,156,96,163,97,97,99,125,100,161,101,101,103,103,104,104,105,34,120,107,155,165,140,127,130,108,108,35,35,35,109,109,128,128,36,36,36,36,36,36,36,36,36,36,36,36,153,19,112,113,4,4,151,20,12,114,114,123,123,129,131,37,21,115,115,115,115,116,154],\"value\":[10,9.09090909090909,8.18181818181818,8.18181818181818,8.18181818181818,7.27272727272727,5.45454545454545,5.45454545454545,4.54545454545455,4.54545454545455,4.54545454545455,4.54545454545455,4.54545454545455,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,3.63636363636364,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,2.72727272727273,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182,1.81818181818182],\"colour\":[\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\",\"#666\"]},\"nodes\":{\"name\":[\"in\",\"of\",\"and\",\"introduction\",\"this\",\"is\",\"such\",\"will\",\"emphasis\",\"laws\",\"linear\",\"on\",\"topics\",\"an\",\"based\",\"course\",\"covered\",\"data\",\"engineering\",\"theory\",\"to\",\"what\",\"abstraction\",\"at\",\"concepts\",\"focus\",\"for\",\"historical\",\"how\",\"image\",\"interest\",\"learn\",\"machine\",\"programming\",\"several\",\"students\",\"the\",\"we\",\"a\",\"about\",\"analysis\",\"answers\",\"applications\",\"approaches\",\"are\",\"art\",\"artificial\",\"as\",\"by\",\"c\",\"can\",\"computer\",\"context\",\"demonstrations\",\"differential\",\"discussion\",\"discussions\",\"drawn\",\"ee\",\"elements\",\"first\",\"fostered\",\"freedom\",\"from\",\"future\",\"graph\",\"group\",\"health\",\"ideas\",\"implications\",\"include\",\"initiative\",\"interaction\",\"interactive\",\"knowledge\",\"language\",\"learning\",\"least\",\"lecture\",\"lectures\",\"limited\",\"math\",\"mathematics\",\"maxwell's\",\"may\",\"modeling\",\"models\",\"mutual\",\"networks\",\"neural\",\"object\",\"order\",\"peer\",\"physical\",\"preference\",\"problem\",\"provides\",\"race\",\"random\",\"reading\",\"role\",\"science\",\"scientific\",\"sections\",\"seeds\",\"sets\",\"skills\",\"software\",\"student\",\"subject\",\"symmetric\",\"their\",\"these\",\"thinking\",\"understanding\",\"with\",\"working\",\"be\",\"algebra\",\"performance\",\"social\",\"do\",\"classification\",\"variables\",\"matrix\",\"research\",\"intelligence\",\"stanford\",\"systems\",\"vision\",\"structures\",\"visualization\",\"equations\",\"103\",\"principles\",\"more\",\"care\",\"does\",\"america\",\"our\",\"squares\",\"104\",\"particularly\",\"network\",\"oriented\",\"change\",\"computing\",\"cs106a\",\"nature\",\"objects\",\"developing\",\"time\",\"logic\",\"theories\",\"world\",\"solving\",\"processes\",\"developed\",\"leaders\",\"develop\",\"matrices\",\"same\",\"own\",\"questions\",\"make\",\"sophomores\",\"mathematical\"],\"group\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"nodesize\":[90,290,260,20,40,70,10,70,30,10,40,90,30,50,30,50,20,60,50,20,140,20,10,20,20,50,30,10,70,20,20,30,20,20,20,40,260,10,80,20,20,20,40,10,30,10,10,30,20,20,20,30,20,20,10,20,10,10,10,10,20,20,10,20,20,10,20,10,10,20,30,10,20,20,10,20,20,10,20,10,10,10,20,10,10,10,10,20,10,10,10,20,20,40,10,20,10,20,20,10,10,20,10,20,20,10,10,10,30,20,10,10,20,10,20,60,10,30,10,10,10,10,10,20,10,10,10,10,20,10,10,10,20,10,10,10,10,10,10,10,10,10,10,10,10,10,20,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10]},\"options\":{\"NodeID\":\"name\",\"Group\":\"group\",\"colourScale\":\"d3.scaleOrdinal(d3.schemeCategory20);\",\"fontSize\":12,\"fontFamily\":\"serif\",\"clickTextSize\":30,\"linkDistance\":50,\"linkWidth\":\"function(d) { return Math.sqrt(d.value); }\",\"charge\":-30,\"opacity\":0.9,\"zoom\":true,\"legend\":false,\"arrows\":false,\"nodesize\":true,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":false,\"opacityNoHover\":1,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]} It’s so much fun to play around with the D3 graph visualization. Hover over nodes to read the labels more clearly. Unsurprisingly, the words cluster around the three nodes: “and”, “the” and “of”, which makes it more difficult to see relationships between domain terms. It’s fun to look at the bigrams that aren’t part of the main connected component, like “machine learning”, “maxwell’s differential equations”, “health care” and “neural networks”. Again, not surprising but kind of cute.\n ","date":1551312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551312000,"objectID":"55db57bede7cc999f714cd6f50f4bd11","permalink":"/post/2019-02-28-visualizing-course-descriptions/","publishdate":"2019-02-28T00:00:00Z","relpermalink":"/post/2019-02-28-visualizing-course-descriptions/","section":"post","summary":"As my senior year at Stanford nears the end, I’ve started to think more and more about what I’ve really learned here, and what I’ll be taking away from my undergraduate experience. Sure, there’s plenty of sappy stuff about coming of age and figuring out what I want to start doing with my life (spoiler: haven’t figured it out), but what about concrete knowledge I’ve gained from my classes.","tags":["R","data-visualization","text-analysis","D3","tidyverse"],"title":"Visualizing Course Descriptions","type":"post"},{"authors":null,"categories":null,"content":"Background\nSimVascular is an open-source software project partly based in the Stanford Cardiovascular Biomechanics Computation Lab, where I spent the Summer of 2018 with support from the Stanford Bioengineering REU program. SimVascular enables segmentation of medical image data to develop 3-dimensional cardiovascular models, which can then be used for patient-specific simulation and analysis of bloodflow.\nOverview\nMy summer project involved developing a GUI plug-in for the software that would allow researchers to rapidly prototype and run simulations on lumped parameter models of the cardiovascular system. Lumped parameter modelling uses electrical circuit diagrams as an analogy to the cardiovascular system, allowing analysis of fundamental quantities such as pressure and flow. These models are often used in early stages of model development, as the simulations are relatively computationally inexpensive. They are also often used to provide boundary conditions to more complex, 3-D models.\nMotivation\nUnder the current architecture, researchers had to manually derive the ODEs that governed their LPN models, and write custom simulation code. This was extremely tedious for large models. In order to use lumped parameter models as boundary conditions, researchers needed to edit FORTRAN code. My goal was to streamline this process by designing a simple drag-and-drop graphical user interface within SimVascular. This interface allows researchers to quickly and easily design models, run simulations, and visualize the results all within the easy-to-use plug-in.\nExecution\nThe simulator plug-in is written in C++ with Qt for easy integration into the existing SimVascular architecture. Promising future directions involve specialization of the simulator for cardiovascular modelling by including large components such as a Windkessel model or even a \u0026ldquo;heart model\u0026rdquo;, which are composed of multiple traditional electrical components.\n","date":1550217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550217600,"objectID":"af2721847a53710fddd7ffe75386e508","permalink":"/project/simvascular/","publishdate":"2019-02-15T00:00:00-08:00","relpermalink":"/project/simvascular/","section":"project","summary":"A plug-in widget for fast and easy lumped parameter modelling of the cardiovascular system in SimVascular.","tags":["cbcl","cardiovascular","software","simulation"],"title":"A SimVascular Plug-in","type":"project"},{"authors":null,"categories":null,"content":"LiCoRICE is an open-source, realtime computation platform specialized for systems neuroscience experimentation. It has been developed in the Brain Interfacing Lab is now the system used to run the group\u0026rsquo;s closed-loop brain interface experiments. The platform is written in C and Python, and is specifically designed with a modular architecture to allow easy prototyping of models by experimenters using only Python.\nI joined the LiCoRICE project in the Summer of 2017. My role has been focused on specializing the platform for systems neuroscience, which has included:\n Testing and implementing graphics (using Pygame) and USB joystick input to the system Developing a realistic systems neuroscience simulation experiment. This includes writing LiCoRICE modules to generate synthetic neural activity from joystick movement input and to decode this synthetic neural data using a pre-trained Kalman filter with steady state gain. Organizing, executing and collecting data from a human trial of the simulation with 7 human participants Analysis of the experimental data and comparison to previous findings to validate the system Contribution to the conference paper published in LCTES\u0026rsquo; 18, and more significantly to a second publication currently in progress, focusing on the systems neuroscience applications of the system.  ","date":1550217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550217600,"objectID":"7d9d68cbd79ca14c3b0d23aa0c43ad61","permalink":"/project/licorice/","publishdate":"2019-02-15T00:00:00-08:00","relpermalink":"/project/licorice/","section":"project","summary":"An open-source realtime computation platform for sytems neuroscience experimentation","tags":["licorice","software","neuroscience","realtime"],"title":"LiCoRICE","type":"project"},{"authors":null,"categories":null,"content":"Students in Data Challenge Lab, the data science intensive course taught by Bill Behrman and Hadley Wickham, pursue a quarter-long data exploration and analysis project of their choice using the Tidyverse R skills they learn in the course. My project explored the dataset released by the Stanford Education Data Archive along with the publication The Geography of Racial/Ethic Test Score Gaps, (Reardon, Kalogrides and Shores, 2017). Using a number of creative data visualization techniques, including some inspired by the New York Times interactive based on the paper, I was able to explore the education achievement gap between students of different races and with parents of different income and education levels geographically, by subject and by student age.\nI am unable to share any results or code in accordance with the class policy and the Stanford Honor Code.\n","date":1550217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550217600,"objectID":"c22a8b467246e223efa12c59f8765cae","permalink":"/project/race-educ/","publishdate":"2019-02-15T00:00:00-08:00","relpermalink":"/project/race-educ/","section":"project","summary":"Exploring disparities between educational outcomes across socioeconomic and racial divides in the United States with data.","tags":["data-science","eda","rstats","education"],"title":"Race, Wealth and Education in America","type":"project"},{"authors":null,"categories":null,"content":"In the Fall of 2018, I worked with the researchers at the Stanford Open Policing Project (OPP) in the Stanford Computational Policy Lab, to implement and run the Veil of Darkness Test, as proposed by Grogger \u0026amp; Ridgeway in 2006 using the large OPP dataset. The premise of the test is that the race of a driver is less visible to the police when it is dark outside than when it is light. Therefore, in the presence of racial bias, there should be a greater disparity in the racial breakdown of drivers stopped during daylight hours than at night. There are a number of factors to control for, most critically time of day, location of the stop.\nMy responsibilities included:\n Finding states and cities for which the data had the necessary covariates for the test, e.g. driver race and stop location Developing raw data visualizations to provide context and intuition for regression models Implementing the test in R for a large dataset aggregated across cities and states Analysing the results of the visualizations and regression models and drawing conclusions about the existence of racial bias and the statistical strengths and limitations of the method  ","date":1550217600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550217600,"objectID":"c09d41aeb416ace6ede28d4f13dd0ac5","permalink":"/project/opp/","publishdate":"2019-02-15T00:00:00-08:00","relpermalink":"/project/opp/","section":"project","summary":"Identifying racial bias in police traffic stop patterns across the United States.","tags":["rstats","statistics","data-science","data","bias","opp"],"title":"The Stanford Open Policing Project","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536476400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536476400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-07:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Pavan Mehrotra*","Sabar Dasgupta*","Samantha Robertson","Paul Nuyujukian"],"categories":null,"content":"","date":1523775600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523775600,"objectID":"d0e261070d317dc32b385019c3562491","permalink":"/publication/licorice-one/","publishdate":"2018-04-15T00:00:00-07:00","relpermalink":"/publication/licorice-one/","section":"publication","summary":"Systems neuroscience studies involving in-vivo models often require realtime data processing. In these studies, many events must be monitored and processed quickly, including behavior of the subject (e.g., movement of a limb) or features of neural data (e.g., a neuron transmitting an action potential). Unfortunately, most realtime platforms are proprietary, require specific architectures, or are limited to low-level programming languages. Here we present a hardware-independent, open-source realtime computation platform that supports high-level programming. The resulting platform, LiCoRICE, can process on order 10e10 bits/sec of network data at 1 ms ticks with 18.2 µs jitter. It connects to various inputs and outputs (e.g., DIO, Ethernet, database logging, and analog line in/out) and minimizes reliance on custom device drivers by leveraging peripheral support via the Linux kernel. Its modular architecture supports model-based design for rapid prototyping with C and Python/Cython and can perform numerical operations via BLAS/LAPACKoptimized NumPy that is statically compiled via Numba’s pycc. LiCoRICE is not only suitable for systems neuroscience research, but also for applications requiring closed-loop realtime data processing from robotics and control systems to interactive applications and quantitative financial trading.","tags":["licorice","neuroscience","software","realtime"],"title":"An Open-Source Realtime Computation Platform","type":"publication"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483257600,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-08:00","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]